{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c883fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'demoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdemoji\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'demoji'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import demoji\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5564a5ca",
   "metadata": {},
   "source": [
    "# Load DataSet and Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_dir = \n",
    "Dataset_name = 'TechLabsDataset.csv' \n",
    "DataDictionary_name = 'TechLabsDataset_Dictionary.csv'\n",
    "\n",
    "Dataset = pd.read_csv(os.path.join('./Data/',Dataset_name), index_col = 0)\n",
    "DataDictionary = pd.read_csv(os.path.join('./Data/',DataDictionary_name), index_col = 0)\n",
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeaec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.DataFrame(Dataset[~Dataset['review_EN'].isnull()]['review_EN'])\n",
    "review_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find emo\n",
    "def find_emo(text):\n",
    "    # find emoji and add to list\n",
    "    all_emoji = demoji.findall(text)\n",
    "    emo = ''\n",
    "    if len(all_emoji)>0:\n",
    "        for i,j in enumerate(all_emoji):\n",
    "            # list emo\n",
    "            emo = emo+','+all_emoji[j]\n",
    "    return emo\n",
    "\n",
    "\n",
    "# replace emo with empty text\n",
    "def replace_emo(text):\n",
    "    # find emo and replce with empty text\n",
    "    all_emoji = demoji.findall(text)\n",
    "\n",
    "    if len(all_emoji)>0:\n",
    "        for i,j in enumerate(all_emoji):\n",
    "            text = text.replace(j,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add emo\n",
    "review_df['emo'] = review_df['review_EN'].apply(lambda x: find_emo(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59411bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add review with no emo\n",
    "review_df['review_no_emo'] = review_df['review_EN'].apply(lambda x: replace_emo(x))\n",
    "# review_df[review_df['emo']!=''][['review_no_emo','emo']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17427550",
   "metadata": {},
   "source": [
    "# NLP pre-processing on review_df['review_no_emo']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f0781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get clean lemmas\n",
    "def get_clean_lemma(text):  \n",
    "    text = [token.lemma_ for token in nlp(text.lower()) if\n",
    "                  not token.is_punct\n",
    "                  and not token.is_currency\n",
    "                  and not token.is_digit\n",
    "                  and not token.is_punct\n",
    "    #               and not token.is_oov# is out of vocabulary\n",
    "                  and not token.is_space\n",
    "                  and not token.is_stop\n",
    "                  and not token.like_num\n",
    "                  and not token.pos_== 'PUNCT'\n",
    "                          ]\n",
    "    return text\n",
    "\n",
    "\n",
    "# # get tags per token\n",
    "# def get_tag_lemma(text): \n",
    "#     tag_list = [token.pos_ for token in nlp(text.lower()) if\n",
    "#                   not token.is_punct\n",
    "#                   and not token.is_currency\n",
    "#                   and not token.is_digit\n",
    "#                   and not token.is_punct\n",
    "#     #               and not token.is_oov# is out of vocabulary\n",
    "#                   and not token.is_space\n",
    "#                   and not token.is_stop\n",
    "#                   and not token.like_num\n",
    "#                           ]\n",
    "#     # ['NN','NNS','NNP','NNPS'] nouns\n",
    "#     # ['JJ','JJR','JJS'] adjectives\n",
    "#     # ['RB','RBR','RBS'] adverbs\n",
    "#     # ['VB','VBD','VBG','VBN','VBP','VBZ'] verbs\n",
    "#     return tag_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab2921",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check attribute of token in spacy: https://spacy.io/api/token\n",
    "\n",
    "# list of stop words in spacy\n",
    "# spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "# print('First ten stop words: %s' % list(spacy_stopwords))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "review_df['text_lemmas'] = review_df['review_no_emo'].apply(lambda x: get_clean_lemma(x))\n",
    "# review_df['lemmas_tags'] = review_df['review_no_emo'].apply(lambda x: get_tag_lemma(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4778309",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concatenate_all_tokens = sum(review_df['text_lemmas'].tolist(),[])\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "word_counts = Counter(concatenate_all_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df = pd.DataFrame.from_dict(word_counts, orient='index').reset_index()\n",
    "word_counts_df.columns = ['word','#']\n",
    "word_counts_df.sort_values('#',ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df['tag'] = word_counts_df['word'].apply(lambda x: nlp(x)[0].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc8936",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(word_counts_df['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_counts_df[word_counts_df['tag']=='X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504837b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df['tag_short'] = word_counts_df['tag'].apply(lambda x: tag_short(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce752562",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df[word_counts_df['tag_short']=='adverbs']['word'].tolist()\n",
    "['ea','p.m.','not','a.m.','ear','har','eurostar','tattoo','semi','ex-','right.only',\n",
    " 'there.fast','gtv','bezeike','alphen','off.a','cbk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbced4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df[word_counts_df['tag_short']=='verbs']['word'].tolist()\n",
    "# ['ea','p.m.','not','a.m.','ear','har','eurostar','tattoo','semi','ex-','right.only',\n",
    "#  'there.fast','gtv','bezeike','alphen','off.a','cbk']\n",
    "['netherlands','cmr','ref','den','fl','tci','lug','b.v','kamado','luxembourg','cm','bo','bah','bio','m','nico',\n",
    "'logistics&move','ist','lorentaweg','sweden','co','amir',\n",
    " 'rabote','fajn','om','cove',\n",
    " 'leadl''patrick',\n",
    " 'toplogistiek','toitoi','swerve','lkw','aceo',\n",
    " 'melden','ce',\n",
    " 'satay',\n",
    " 'manfre',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56448f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df[word_counts_df['tag_short']=='nouns']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
